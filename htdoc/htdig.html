<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd">
<html>
  <head>
	<title>
	  ht://Dig: htdig
	</title>
  </head>
  <body bgcolor="#eef7ff">
	<h1>
	  htdig
	</h1>
	<p>
	  ht://Dig Copyright &copy; 1995-2000 The ht://Dig Group<br>
	  Please see the file <a href="COPYING">COPYING</a> for
	  license information.
	</p>
	<hr size="4" noshade>
	<dl>
	  <dd>
		<h2>
		  Synopsis
		</h2>
	  </dd>
	  <dd>
		htdig [<em>options</em>]
	  </dd>
	</dl>
	<dl>
	  <dd>
		<h2>
		  Description
		</h2>
	  </dd>
	  <dd>
		Htdig retrieves HTML documents using the HTTP protocol and
		gathers information from these documents which can later be
		used to search these documents. This program can be
		referred to as the search robot.
	  </dd>
	</dl>
	<dl>
	  <dd>
		<h2>
		  Options
		</h2>
	  </dd>
	  <dd>
		<dl compact>
	          <dt>
		        -
		  </dt>
		  <dd>
		         Get the list of URLs to start indexing from
		         the STDIN. This will override the default
		         start_url and the file supplied to -m [url_file].
		  </dd>
		  <dt>
			-a
		  </dt>
		  <dd>
			Use alternate work files. Tells htdig to append <em>
			.work</em> to database files, causing a second copy of
			the database to be built. This allows the original
			files to be used by htsearch during the indexing run. When
			used without the "-i" flag for an update dig, htdig will
			use any existing .work files for the databases to update.
		  </dd>
		  <dt>
			-c <em>configfile</em>
		  </dt>
		  <dd>
			Use the specified <em>configfile</em> file instead of the
			default.
		  </dd>
		  <dt>
			-h <em>maxhops</em>
		  </dt>
		  <dd>
			Restrict the dig to documents that are at most <em>
			maxhops</em> links away from the starting document.
		  </dd>
		  <dt>
			-i
		  </dt>
		  <dd>
			Initial. Do not use any old databases. This is
			accomplished by first erasing the databases.
		  </dd>
		  <dt>
			-l
		  </dt>
		  <dd>
			 Stop and restart. Reads in the progress of any previous
			 interrupted digs from the
			 <a href="attrs.html#url_log">log file</a> and write the
			 progress out if interrupted by a signal.
		  </dd>
		  <dt>
		      -m [url_file]
		  </dt>
		  <dd>
		         Minimal. Only index the URLs in the file provided and no others.
		  </dd>
		  <dt>
			-s
		  </dt>
		  <dd>
			Print statistics about the dig after completion.
		  </dd>
		  <dt>
			-t
		  </dt>
		  <dd>
			Create an ASCII version of the document database. This
			database is easy to parse with other programs so that
			information can be extracted from it for purposes other
			than searching. One could gather some interesting
			statistics from this database.
			<p>Each line in the file starts with the document id 
			followed by a list of
			<strong>\t<em>fieldname</em>:<em>value</em></strong>.
			The fields always appear in the order listed bellow:
			</p>
			<table border=0>
			<tr> <th>fieldname</th><th>value</th></tr>
			<tr> <td>u</td><td>URL</td></tr>
			<tr> <td>t</td><td>Title</td></tr>
			<tr> <td>a</td><td>State (0 = normal, 1 = not found, 2
			= not indexed, 3 = obsolete)</td></tr>
			<tr> <td>m</td><td>Last modification time as reported
			by the server</td></tr> 
			<tr> <td>s</td><td>Size in bytes</td></tr>
			<tr> <td>H</td><td>Excerpt</td></tr>
			<tr> <td>h</td><td>Meta description</td></tr>
			<tr> <td>l</td><td>Time of last retrieval</td></tr>
			<tr> <td>L</td><td>Count of the links in the document
			(outgoing links)</td></tr>
			<tr> <td>b</td><td>Count of the links to the document
			(incoming links or backlinks)</td></tr>
			<tr> <td>c</td><td>HopCount of this document</td></tr>
			<tr> <td>g</td><td>Signature of the document used for
			duplicate-detection</td></tr>
			<tr> <td>e</td><td>E-mail address to use for a
			notification message from htnotify</td></tr>
			<tr> <td>n</td><td>Date to send out a notification
			e-mail message</td></tr>
			<tr> <td>S</td><td>Subject for a notification e-mail
			message</td></tr>
			<tr> <td>d</td><td>The text of links pointing to this
			document. (e.g. &lt;a
			href=&quot;docURL&quot;&gt;description&lt;/a&gt;)</td></tr>
			<tr> <td>A</td><td>Anchors in the document (i.e. &lt;A
			NAME=...)</td></tr>
			</table>
		  </dd>
		  <dt>
			-u <em>username:password</em>
		  </dt>
		  <dd>
			Tells htdig to send the supplied username and password
			with each HTTP request. The credentials will be encoded
			using the 'Basic' authentication scheme. There <strong>
			HAS</strong> to be a colon (:) between the username and
			password.
		  </dd>
		  <dt>
			-v
		  </dt>
		  <dd>
			Verbose mode. This increases the verbosity of the
			program. Using more than 2 is probably only useful for
			debugging purposes. The default verbose mode (using
			only one -v) gives a nice progress report while
			digging.
		  </dd>
		</dl>
	  </dd>
	</dl>
	<dl>
	  <dd>
		<h2>
		  Files
		</h2>
	  </dd>
	  <dd>
		<dl>
		  <dt>
			CONFIG_DIR/htdig.conf
		  </dt>
		  <dd>
			The default configuration file.
		  </dd>
		</dl>
	  </dd>
	</dl>
	<dl>
	  <dd>
		<h2>
		  See Also
		</h2>
	  </dd>
	  <dd>
		<a href="htmerge.html">htmerge</a>,
		<a href="htsearch.html" target="_top">htsearch</a>,
		<a href="attrs.html">Configuration file format</a>, and
		<a href="http://info.webcrawler.com/mak/projects/robots/norobots.html">
		A Standard for Robot Exclusion</a>.
	  </dd>
	</dl>
	<hr size="4" noshade>
	<address>
	  <a href="author.html">Andrew Scherpbier &lt;andrew@contigo.com&gt;</a>
	</address>

Last modified: $Date: 2000/04/09 15:15:04 $

  </body>
</html>
